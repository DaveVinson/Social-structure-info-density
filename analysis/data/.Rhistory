tot_uni  = tot_uni + length(words)
}
k
x[[k]]$full_text
tail(unlist(strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE), use.names = FALSE),-3);
unlist(strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE), use.names = FALSE),-3);
unlist(strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE), use.names = FALSE)
w <- unlist(strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE), use.names = FALSE)
w
View(means)
w <- unlist(strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE), use.names = FALSE)
w
as.character(x[[k]]$full_text
)
character(x[[k]]$full_text)
character(x[[k]]$full_text)
as.character(x[[k]]$full_text)
trsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE)
strsplit(as.character(x[[k]]$full_text), " ", fixed = TRUE)
x[[k]]$full_text
iconv(x[[k]]$full_text,"latin1", "UTF-8")
w <- unlist(strsplit(as.character(iconv(x[[k]]$full_text,"latin1", "UTF-8")), " ", fixed = TRUE), use.names = FALSE)
w
w <- unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), " ", fixed = TRUE), use.names = FALSE)
w
str <- toupper(line);
# strip-out small html tags
str <- gsub('<[^>]{1,2}>', '', str);
# replace all terminal punctuation with a period
str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '.', str);
# get rid of anything not A-Z, ', ., or whitespace
str <- gsub('[^A-Z\'.[:space:]]', ' ', str);
# crowd apostrophes
str <- gsub("[[:space:]]+([A-Z]*'[A-Z]*)", "\\1", str);
# collapse whitespace
str <- gsub('[[:space:]]+', ' ', str);
# make sure contraction's are "tight"
str <- gsub(" ?' ?", "'", str);
# make sure terminal . are tight
str <- gsub(' ?\\. ?', '.', str);
return(str);
}
clean <- function(line) {
# upper-case everything
str <- toupper(line);
# strip-out small html tags
str <- gsub('<[^>]{1,2}>', '', str);
# replace all terminal punctuation with a period
str <- gsub('[[:space:]]*[.?!:;]+[[:space:]]*', '.', str);
# get rid of anything not A-Z, ', ., or whitespace
str <- gsub('[^A-Z\'.[:space:]]', ' ', str);
# crowd apostrophes
str <- gsub("[[:space:]]+([A-Z]*'[A-Z]*)", "\\1", str);
# collapse whitespace
str <- gsub('[[:space:]]+', ' ', str);
# make sure contraction's are "tight"
str <- gsub(" ?' ?", "'", str);
# make sure terminal . are tight
str <- gsub(' ?\\. ?', '.', str);
return(str);
}
w <- unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), " ", fixed = TRUE), use.names = FALSE)
w
clean(iconv(x[[k]]$full_text,"latin1", "UTF-8"))
w <- unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), use.names = FALSE)
w
strsplit(w," ",fixed=T)
w <- unlist(strsplit(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE)," ",fixed=TRUE), use.names = FALSE)
w <- unlist(strsplit(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), " ",fixed=TRUE), use.names = FALSE)
strsplit(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), " ",fixed=TRUE)
unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), use.names = FALSE)
w <- strsplit(unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), use.names = FALSE)," ",fixed=TRUE)
w
w <- unlist(strsplit(unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), use.names = FALSE)," ",fixed=TRUE), use.names = FALSE)
w
w <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")), ".", fixed = TRUE), use.names = FALSE)," ",fixed=TRUE), use.names = FALSE),-2)
w
means <- matrix(ncol=13, nrow=length(x))
#info loop(s)
dictuni <- new(FrequencyDictionary, 4, 2 ^ 26)
dictbi <- new(FrequencyDictionary, 4, 2 ^ 26) #inside the loop for entropy, outside for information
dicttri <- new(FrequencyDictionary, 4, 2 ^ 28)
tot_uni=0
#tot_bi = 0
#tot_tri = 0
#build the dicts
for (k in 1:length(x)){
print(k)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
trigrams <- do.call(paste, list(head(words, -1), tail(words, -1), tail(words,-2)));
dictuni$store(words);
dictbi$store(bigrams);
dicttri$store(trigrams);
#for next loop
tot_uni  = tot_uni + length(words)
}
#get info measures
for (i in 1:length(x)){
print(i)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[i]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
#get bigrams
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
#get trigrams
trigrams <- do.call(paste, list(head(words, -2), head(tail(words,-1),-1), tail(words,-2)));
means[i,1:12]<-c(i,x[[i]]$received_pizza,length(words),
mean(-log2(dictuni$query(words)/tot_uni)), #unigram probabiilty (collapse conditional and joint)
mean(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))), #conditional bigram probability
mean(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))), #conditional trigram probability
min(-log2(dictuni$query(words)/tot_uni)),
max(-log2(dictuni$query(words)/tot_uni)),
-log2(min(dictbi$query(bigrams))/max(dictuni$query(head(words,-1)))),
min(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))),
-log2(min(dicttri$query(trigrams))/max(dictbi$query(head(bigrams,-1)))),
min(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))));
if (x[[i]]$feature_n_emoticons>0){
means[i,13]=c(x[[i]]$full_text)
}
}
View(means)
length(means$V1)
length(means[[1]])
length(means[,1])
x[[i]]$received_pizza
as.character(x[[i]]$received_pizza)
means <- matrix(ncol=13, nrow=length(x))
#info loop(s)
dictuni <- new(FrequencyDictionary, 4, 2 ^ 26)
dictbi <- new(FrequencyDictionary, 4, 2 ^ 26) #inside the loop for entropy, outside for information
dicttri <- new(FrequencyDictionary, 4, 2 ^ 28)
tot_uni=0
#tot_bi = 0
#tot_tri = 0
#build the dicts
for (k in 1:length(x)){
print(k)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[k]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
trigrams <- do.call(paste, list(head(words, -1), tail(words, -1), tail(words,-2)));
dictuni$store(words);
dictbi$store(bigrams);
dicttri$store(trigrams);
#for next loop
tot_uni  = tot_uni + length(words)
}
#get info measures
for (i in 1:length(x)){
print(i)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[i]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
#get bigrams
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
#get trigrams
trigrams <- do.call(paste, list(head(words, -2), head(tail(words,-1),-1), tail(words,-2)));
means[i,1:12]<-c(i,as.character(x[[i]]$received_pizza),length(words),
mean(-log2(dictuni$query(words)/tot_uni)), #unigram probabiilty (collapse conditional and joint)
mean(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))), #conditional bigram probability
mean(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))), #conditional trigram probability
min(-log2(dictuni$query(words)/tot_uni)),
max(-log2(dictuni$query(words)/tot_uni)),
-log2(min(dictbi$query(bigrams))/max(dictuni$query(head(words,-1)))),
min(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))),
-log2(min(dicttri$query(trigrams))/max(dictbi$query(head(bigrams,-1)))),
min(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))));
if (x[[i]]$feature_n_emoticons>0){
means[i,13]=c(x[[i]]$full_text)
}
}
View(means)
means <- matrix(ncol=13, nrow=length(x))
#get info measures
for (i in 1:length(x)){
print(i)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[i]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
#get bigrams
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
#get trigrams
trigrams <- do.call(paste, list(head(words, -2), head(tail(words,-1),-1), tail(words,-2)));
means[i,1:12]<-c(i,as.character(x[[i]]$received_pizza),x[[i]]$feature_n_emoticons,length(words),
mean(-log2(dictuni$query(words)/tot_uni)), #unigram probabiilty (collapse conditional and joint)
mean(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))), #conditional bigram probability
mean(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))), #conditional trigram probability
min(-log2(dictuni$query(words)/tot_uni)),
max(-log2(dictuni$query(words)/tot_uni)),
-log2(min(dictbi$query(bigrams))/max(dictuni$query(head(words,-1)))),
min(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))),
-log2(min(dicttri$query(trigrams))/max(dictbi$query(head(bigrams,-1)))),
min(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))));
if (x[[i]]$feature_n_emoticons>0){
means[i,13]=c(x[[i]]$full_text)
}
}
means <- matrix(ncol=14, nrow=length(x))
#get info measures
for (i in 1:length(x)){
print(i)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x[[i]]$full_text,"latin1", "UTF-8")),
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
#get bigrams
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
#get trigrams
trigrams <- do.call(paste, list(head(words, -2), head(tail(words,-1),-1), tail(words,-2)));
means[i,1:13]<-c(i,as.character(x[[i]]$received_pizza),x[[i]]$feature_n_emoticons,length(words),
mean(-log2(dictuni$query(words)/tot_uni)), #unigram probabiilty (collapse conditional and joint)
mean(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))), #conditional bigram probability
mean(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))), #conditional trigram probability
min(-log2(dictuni$query(words)/tot_uni)),
max(-log2(dictuni$query(words)/tot_uni)),
-log2(min(dictbi$query(bigrams))/max(dictuni$query(head(words,-1)))),
min(-log2(dictbi$query(bigrams)/dictuni$query(head(words,-1)))),
-log2(min(dicttri$query(trigrams))/max(dictbi$query(head(bigrams,-1)))),
min(-log2(dicttri$query(trigrams)/dictbi$query(head(bigrams,-1)))));
if (x[[i]]$feature_n_emoticons>0){
means[i,14]=c(x[[i]]$full_text)
}
}
View(means)
colnames(means)=c("request","pizza","n-emoticons" "len","uninfo","binfo","trinfo",
"lo_uninfo","hi_uninfo","lo_binfo","hi_binfo","lo_trinfo","hi_triinfo","emotxt")
colnames(means)=c("request","pizza","n-emoticons" "len","uninfo","binfo","trinfo",
"lo_uninfo","hi_uninfo","lo_binfo","hi_binfo","lo_trinfo","hi_triinfo","emotxt")
colnames(means)=c("request","pizza","n-emoticons", "len","uninfo","binfo","trinfo",
"lo_uninfo","hi_uninfo","lo_binfo","hi_binfo","lo_trinfo","hi_triinfo","emotxt")
View(means)
write.csv(means,file="~/Desktop/pizza-info-emotxt.csv")
m = as.data.fram(means)
m = as.data.frame(means)
View(m)
write.csv(m,file="~/Desktop/pizza-info-emotxt.csv")
View(m)
cor.test(m$`n-emoticons`,m$binfo)
cor.test(m$n-emoticons,m$binfo)
cor.test(as.numeric(m$`n-emoticons`),m$binfo)
cor.test(as.numeric(m$`n-emoticons`),as.numeric(m$binfo))
plot(as.numeric(m$`n-emoticons`),as.numeric(m$binfo))
plot(as.numeric(m$`n-emoticons`),as.numeric(m$uninfo))
cor.test(as.numeric(m$`n-emoticons`),as.numeric(m$uninfo))
cor.test(as.numeric(m$`n-emoticons`),as.numeric(m$trinfo))
hist(m$trinfo)
hist(as.numeric(m$trinfo))
as.numeric(m$trinfo)
cor.test(as.numeric(m$`n-emoticons`),as.value(m$trinfo))
View(means)
means$binfo
as.numeric(means$binfo)
means[binfo]
means[,1]
means[,4]
means[,binfo]
m$binfo
as.numeric(m$binfo)
as.numeric(as.character(m$binfo))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$binfo)))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$uninfo)))
as.numeric(as.character(m$uninfo))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$uninfo)))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$trinfo)))
as.numeric(as.character(m$trinfo)
(as.character(m$trinfo))
)
as.character(m$`n-emoticons`)
as.numeric(as.character(m$`n-emoticons`))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$trinfo)))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$uninfo)))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$trinfo)))
hist(as.numeric(as.character(m$trinfo)))
hist(as.numeric(as.character(m$trinfo))
hist(as.numeric(as.character(m$trinfo)))
hist(as.numeric(as.character(m$trinfo))
)
hist(log(as.numeric(as.character(m$trinfo))))
hist(as.numeric(as.character(m$trinfo)))
hist(as.numeric(as.character(m$uninfo)))
hist(log(as.numeric(as.character(m$uninfo))))
cor.test(as.numeric(as.character(m$`n-emoticons`)),log(as.numeric(as.character(m$uninfo))))
cor.test(as.numeric(as.character(m$`n-emoticons`)),log(as.numeric(as.character(m$binfo))))
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$trinfo)))
View(m)
x[[i]]
cor.test(as.numeric(as.character(m$`n-emoticons`)),as.numeric(as.character(m$trinfo)))
x<-read.csv('Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv')
x<-read.csv('/Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv')
View(x)
x['new']<c()
x$newwss<c()
x$newwwww=c()
x$ffffff = c()
View(x)
x$newwwww=0
x<-read.csv('/Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv')
#info loop(s)
dictuni <- new(FrequencyDictionary, 4, 2 ^ 26)
dictbi <- new(FrequencyDictionary, 4, 2 ^ 26) #inside the loop for entropy, outside for information
dicttri <- new(FrequencyDictionary, 4, 2 ^ 28)
tot_uni=0
#tot_bi = 0
#tot_tri = 0
#build the dicts
x[[k]]$full_text
x[[1]]$full_text
x[1]$full_text
View(x)
x$full_text[1]
x$full_text[[1]]
x$full_text[1,]
x$full_text[[1]][1]
x$full_text[[1]][1]
x[[1]][[1]]
x[[1]]
x[[1]][1]
x[1][[1]]
x<-data.table(read.csv('/Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv'))
library(data.table)
x<-data.table(read.csv('/Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv'))
head(x)
x$full_text
x$full_text[1]
x$full_text = as.character(x$full_text)
x$full_text[1]
x$full_text[k]
#info loop(s)
dictuni <- new(FrequencyDictionary, 4, 2 ^ 26)
dictbi <- new(FrequencyDictionary, 4, 2 ^ 26) #inside the loop for entropy, outside for information
dicttri <- new(FrequencyDictionary, 4, 2 ^ 28)
tot_uni=0
#tot_bi = 0
#tot_tri = 0
#build the dicts
for (k in 1:length(x)){
print(k)
words <- tail(unlist(strsplit(unlist(strsplit(clean(iconv(x$full_text[k],"latin1", "UTF-8")), #x[[k]]$full_text
".", fixed = TRUE),
use.names = FALSE),
" ",fixed=TRUE), use.names = FALSE),-2);
bigrams <- do.call(paste, list(head(words, -1), tail(words, -1)));
trigrams <- do.call(paste, list(head(words, -1), tail(words, -1), tail(words,-2)));
dictuni$store(words);
dictbi$store(bigrams);
dicttri$store(trigrams);
#for next loop
tot_uni  = tot_uni + length(words)
}
nrow(x)
x<-data.table(read.csv('/Users/Dave/Dropbox/pizza_request_dataset/Data_2017-01-23.csv'))
x$full_text = as.character(x$full_text)
#info loop(s)
dictuni <- new(FrequencyDictionary, 4, 2 ^ 26)
dictbi <- new(FrequencyDictionary, 4, 2 ^ 26) #inside the loop for entropy, outside for information
dicttri <- new(FrequencyDictionary, 4, 2 ^ 28)
tot_uni=0
#tot_bi = 0
#tot_tri = 0
#build the dicts
??cmscu
require(cmscu);
install.packages("cmscu.tgz",repos=NULL,type="src")
install.packages("~/Users/Dave/Downloads/cmscu.tgz",repos=NULL,type="src")
install.packages("~/Downloads/cmscu.tgz",repos=NULL,type="src")
install.packages("RcppParallel")
install.packages("~/Downloads/cmscu.tgz",repos=NULL,type="src")
require(cmscu);
require(rbenchmark);
require(stringi);
test <- new(FrequencyDictionary, 4, 2^20);
str <- tolower(stringi::stri_rand_strings(12, 100));
test$store(sample(str, size=100000, replace=TRUE));
str2 <- c(str, tolower(stringi::stri_rand_strings(12,1000000)));
# test 1: correctness
print("Correct?");
out1 <- test$query(str2);
out2 <- test$query(str2);
print(all(out1==out2));
# test 2: speed
f1 <- function() { invisible(test$query(str2)) };
f2 <- function() { invisible(test$query(str2)) };
res <- benchmark(f1(), f2(), order="relative");
print(res);
require(cmscu);
require(rbenchmark);
require(stringi);
test <- new(FrequencyDictionary, 4, 2^20);
str <- tolower(stringi::stri_rand_strings(12, 100));
test$store(sample(str, size=100000, replace=TRUE));
str2 <- c(str, tolower(stringi::stri_rand_strings(12,1000000)));
# test 1: correctness
print("Correct?");
out1 <- test$query(str2);
out2 <- test$pquery(str2);
print(all(out1==out2));
# test 2: speed
f1 <- function() { invisible(test$query(str2)) };
f2 <- function() { invisible(test$pquery(str2)) };
res <- benchmark(f1(), f2(), order="relative");
print(res);
efaultNumThreads()
defaultNumThreads()
defaultNumThreads()
defaultNumThreads()
require(RcppParallel)
defaultNumThreads()
setThreadOptions(numThreads = 2)
require(cmscu);
require(rbenchmark);
require(stringi);
test <- new(FrequencyDictionary, 4, 2^20);
str <- tolower(stringi::stri_rand_strings(12, 100));
test$store(sample(str, size=100000, replace=TRUE));
str2 <- c(str, tolower(stringi::stri_rand_strings(12,1000000)));
# test 1: correctness
print("Correct?");
out1 <- test$query(str2);
out2 <- test$pquery(str2);
print(all(out1==out2));
# test 2: speed
f1 <- function() { invisible(test$query(str2)) };
f2 <- function() { invisible(test$pquery(str2)) };
res <- benchmark(f1(), f2(), order="relative");
print(res);
defaultNumThreads()
setThreadOptions(numThreads = 2)
readRDS('~/Downloads/AD data.RData')
readRDS('~/Dave/Downloads/AD data.RData')
readRDS('Users/Dave/Downloads/AD data.RData')
readRDS('/Users/Dave/Downloads/AD data.RData')
readRDS('~/Users/Dave/Downloads/AD data.RData')
readRDS('~Downloads/AD data.RData')
readRDS('~/Downloads/AD data.RData')
readRDS('~/Downloads/AF data.RData')
new<- readRDS('~/Downloads/AF data.RData')
update.packages()
y
new<- readRDS('~/Downloads/AF data.RData')
source('../net.function10.R')  #description in code
rootFolder = '/Users/Dave/Documents/github/Social-structure-info-density/analysis/data/'
setwd(rootFolder)
source('../net.function10.R')  #description in code
View(build_networks)
library(network)
library(jsonlite)
library(igraph)
library(plyr)
us_shuff = us[sample(1:10000),] #shuffles reviews
user_list = as.character(read.csv('~/Desktop/user_list.csv')[,2])#take second column (user_id), first is numbered index
us = read.csv('/Volumes/Seagate/yelp/yelp_dataset-jan 2016/yelp_academic_dataset_user.json',sep='\n',quote = "")
colnames(us) = list('u')
us$u = as.character(us$u)
us_shuff = us[sample(1:10000),] #shuffles reviews
json = fromJSON(us_shuff[1]) #pulls a single review and turns it into a list
for (run in 1:100) {
print(run)
build_networks()
setwd('~/new-networks-100')
save(bsk.network,file = paste("net",run,".RData",sep=""))
save(net_users,file = paste("users",run,".RData",sep=""))
save(edges2,file = paste("edges",run,".RData",sep=""))
}
setwd(rootFolder)
setwd('/new-networks-100')
getwd()
setwd('/new-networks-100/')
setwd('~/new-networks-100/')
setwd('/Users/Dave/Documents/github/Social-structure-info-density/analysis/data/new-networks-100/')
for (run in 1:100) {
print(run)
build_networks()
getwd()
setwd('/Users/Dave/Documents/github/Social-structure-info-density/analysis/data/new-networks-100/')
save(bsk.network,file = paste("net",run,".RData",sep=""))
save(net_users,file = paste("users",run,".RData",sep=""))
save(edges2,file = paste("edges",run,".RData",sep=""))
}
save(bsk.network,file = paste("net",run,".RData",sep=""))
save(net_users,file = paste("users",run,".RData",sep=""))
save(edges2,file = paste("edges",run,".RData",sep=""))
build_networks()
setwd('/Users/Dave/Documents/github/Social-structure-info-density/analysis/data/new-networks-100/')
save(bsk.network,file = paste("net",run,".RData",sep=""))
save(net_users,file = paste("users",run,".RData",sep=""))
save(edges2,file = paste("edges",run,".RData",sep=""))
for (run in 1:100) {
print(run)
build_networks()
setwd('/Users/Dave/Documents/github/Social-structure-info-density/analysis/data/new-networks-100/')
save(bsk.network,file = paste("net",run,".RData",sep=""))
save(net_users,file = paste("users",run,".RData",sep=""))
save(edges2,file = paste("edges",run,".RData",sep=""))
}
setwd(rootFolder)
rand_index1 = read.csv('index_allnets_revs.csv',sep=',')
length(rand_index1)
length(rand_index1$x)
sample(rand_index1,length(rand_index1$x))
rand_index1 = rand_index1$x
rand_index1 = read.csv('index_allnets_revs.csv',sep=',')$x
rand_index = sample(rand_index1,length(rand_index1))
rand_index
